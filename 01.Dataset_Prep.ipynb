{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification of sustainability-focused campaigns on the kickstarter crowdfunding platform using NLP and ML boosted with swarm intelligence\n",
    "--- ------------------\n",
    "<div>\n",
    "Data Analysis: part 1\n",
    "<br>\n",
    "Submitted by: Jossin Antony<br>\n",
    "Affiliation: THU Ulm<br>\n",
    "Date: 11.06.2024\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- [Introduction](#introduction)\n",
    "- [Details of dataset](#details-of-dataset)\n",
    "- [Preparation of Dataset](#Preparation-of-Dataset)\n",
    "- [Save dataset](#save-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "### Introduction\n",
    "--- -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of the project is to study how crowdfunding campaigns support sustainable inititatives. This project, in particular, focuses on crowdfunded campaigns in the [kickstarter](https://www.kickstarter.com/) platform and explores a dataset of c.a 184,186 initiatives from different domains (e.g, Technology, Music, Publishing etc.). The goal of the analyses here is to find the most important features that are relevant to initiatives that are both sustainable as well as profitable. The analyses will also explore the possible relationship of the features with each other, and elucidate insights that might contribute to better understanding of the success/failure propsects of current and future environment focused crowdfunded initiatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"details-of-dataset\"></a>\n",
    "#### Details of dataset:\n",
    "-- -------------------\n",
    "1. Source: [Kickstarter_File.xlsx](Kickstarter_File.xlsx)\n",
    "2. Generation mode: provided by researcher\n",
    "3. Time period considered: 04-2009 to 05-2021 (c.a 146 months).\n",
    "4. Total entries: 184,185\n",
    "\n",
    "The initial data preparation consists of examining the various features and eliminating redundant features & renaming and re-ordering of features and saving the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation of Dataset\n",
    "--- ------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make sure the dataset is 'reasonable', i.e, it has good structure, columns have data of expected types, devoid of null values etc.\n",
    "\n",
    "The basic information of the data is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "from langdetect import detect\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import PrettyPrinter\n",
    "pp=PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the raw dataset from excel file. This is slow.\n",
    "df= pd.read_excel('./data/Kickstarter_File.xlsx')\n",
    "#--------------------------------------------------\n",
    "#write the raw dataset as a data frame in a csv file. Do this only once, if the dataframe file is not provided already.\n",
    "df.dropna(how='all', inplace=True)\n",
    "df.to_csv('./data/dataframe_raw.csv', index=False)\n",
    "#--------------------------------------------------\n",
    "#load the raw dataset from the data frame csv file, ONCE file is already created. This is fast as compared to reading from excel file.\n",
    "#df= pd.read_csv('./data/dataframe_raw.csv', low_memory=False)\n",
    "#df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe has 184187 rows and 24 columns.\n",
      "\n",
      "The overall dataframe information is given below:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 184187 entries, 0 to 1048574\n",
      "Data columns (total 24 columns):\n",
      " #   Column                    Non-Null Count   Dtype         \n",
      "---  ------                    --------------   -----         \n",
      " 0   blurb                     184184 non-null  object        \n",
      " 1   Environmental             2053 non-null    object        \n",
      " 2   Social                    2053 non-null    object        \n",
      " 3   state                     184186 non-null  object        \n",
      " 4   Subcategory               184186 non-null  object        \n",
      " 5   Unnamed: 5                176465 non-null  object        \n",
      " 6   converted_pledged_amount  184186 non-null  float64       \n",
      " 7   country                   184186 non-null  object        \n",
      " 8   country_displayable_name  184186 non-null  object        \n",
      " 9   created_at                184186 non-null  datetime64[ns]\n",
      " 10  currency                  184186 non-null  object        \n",
      " 11  deadline                  184186 non-null  datetime64[ns]\n",
      " 12  fx_rate                   184186 non-null  float64       \n",
      " 13  goal                      184186 non-null  float64       \n",
      " 14  launched_at               184186 non-null  datetime64[ns]\n",
      " 15  duration                  184186 non-null  float64       \n",
      " 16  name                      184186 non-null  object        \n",
      " 17  pledged                   184186 non-null  float64       \n",
      " 18  slug                      184186 non-null  object        \n",
      " 19  staff_pick                184186 non-null  float64       \n",
      " 20  state.1                   184186 non-null  object        \n",
      " 21  static_usd_rate           184186 non-null  float64       \n",
      " 22  usd_exchange_rate         184186 non-null  float64       \n",
      " 23  usd_pledged               184186 non-null  float64       \n",
      "dtypes: datetime64[ns](3), float64(9), object(12)\n",
      "memory usage: 35.1+ MB\n",
      "None\n",
      "\n",
      "We also make the preliminary observation that the columns named 'environmental', 'social' and 'unnamed: 5' have lots of 'NaN' values. We will deal with them later.\n"
     ]
    }
   ],
   "source": [
    "df.rename_axis('index',inplace=True)\n",
    "shape= df.shape\n",
    "print(f'The dataframe has {shape[0]} rows and {shape[1]} columns.')\n",
    "print()\n",
    "print('The overall dataframe information is given below:')\n",
    "print(df.info())\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"We also make the preliminary observation that the columns named 'environmental', 'social' and 'unnamed: 5' \\\n",
    "have lots of 'NaN' values. We will deal with them later.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we provide meaningful names to the columns to reflect the nature of the data they contain as well as re-order them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns to meaningful names\n",
    "df.rename(columns={'Environmental':'is_environmental',\n",
    "                   'Social':'is_social',\n",
    "                   'state':'is_success',\n",
    "                   'Unnamed: 5':'main_category',\n",
    "                   'Subcategory':'sub_category',\n",
    "                   'converted_pledged_amount':'pledged_amount_usd',\n",
    "                   'goal':'goal_in_local_currency',\n",
    "                   'duration':'duration_in_days',\n",
    "                   'name':'campaign_name',\n",
    "                   'pledged':'pledged_in_local_currency',\n",
    "                   },inplace=True)\n",
    "#Reorder the columns\n",
    "df=df[['campaign_name', \n",
    "       'blurb', \n",
    "       'slug', \n",
    "       'main_category',\n",
    "       'sub_category', \n",
    "       'is_environmental', \n",
    "       'is_social', \n",
    "       'country', \n",
    "       'country_displayable_name', \n",
    "       'created_at', \n",
    "       'launched_at', \n",
    "       'deadline', \n",
    "       'duration_in_days', \n",
    "       'currency', \n",
    "       'goal_in_local_currency', \n",
    "       'pledged_in_local_currency', \n",
    "       'usd_pledged',\n",
    "       'pledged_amount_usd', \n",
    "       'staff_pick', \n",
    "       'state.1', \n",
    "       'fx_rate', \n",
    "       'static_usd_rate', \n",
    "       'usd_exchange_rate',\n",
    "       'is_success',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The new column_names are:'\n",
      "['campaign_name',\n",
      " 'blurb',\n",
      " 'slug',\n",
      " 'main_category',\n",
      " 'sub_category',\n",
      " 'is_environmental',\n",
      " 'is_social',\n",
      " 'country',\n",
      " 'country_displayable_name',\n",
      " 'created_at',\n",
      " 'launched_at',\n",
      " 'deadline',\n",
      " 'duration_in_days',\n",
      " 'currency',\n",
      " 'goal_in_local_currency',\n",
      " 'pledged_in_local_currency',\n",
      " 'usd_pledged',\n",
      " 'pledged_amount_usd',\n",
      " 'staff_pick',\n",
      " 'state.1',\n",
      " 'fx_rate',\n",
      " 'static_usd_rate',\n",
      " 'usd_exchange_rate',\n",
      " 'is_success']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint('The new column_names are:');pp.pprint(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we drop the columns which are redundant or which do not add any value to the analysis. The dropped columns are as following:\n",
    "\n",
    "1. <b>'country' and 'country_displayable_name':</b>\n",
    "\n",
    "    We need only one of these; but we save the country codes for later reference.\n",
    "\n",
    "2. <b>'created_at', 'launched_at', 'deadline', 'duration':</b>\n",
    "\n",
    "    There is no discernible difference between 'created_at' and 'launched_at' since they are, at maximum, only few days apart in order to have an effect on the results we look for. 'duration' provides the difference in days between launched_at and deadline and we keep this parameter (for now).\n",
    "\n",
    "3. <b>'currency', 'goal_in_local_currency', 'pledged_in_local_currency', 'usd_pledged','converted_pledged_amount_usd', 'fx_rate', 'static_usd_rate', 'usd_exchange_rate':</b>\n",
    "\n",
    "    There is the goal- but only in local currency- and the pledged amount- in both local currency and usd. \n",
    "    We add a new column, 'goal_in_usd', which gives the goal in usd as well. It is obtained by multiplying the 'goal_in_local_currency' with the provided 'usd_exchange_rate' (Logic: The converted_pledged_amount_usd is provided by the author as a product of 'usd_exchange_rate' and 'pledged_in_local_currency').\n",
    "\n",
    "4. <b>'staff_pick' and 'state.1':</b>\n",
    "    These columns are dropped, since state.1 is a reptition of the column 'is_success' and 'staff_pick' do not seem to add value to the analysis at hand.\n",
    "\n",
    "5. <b>'slug' and 'campaign_name':</b>\n",
    "    'slug'is a repetition of 'campaign_name', it is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns which do not add value to the analysis\n",
    "#---------------------------------------------------\n",
    "#1. 'country' and 'country_displayable_name'.\n",
    "# We need only on of these; but save the country codes for later reference.\n",
    "df[['country_displayable_name','country']].drop_duplicates().reset_index(drop=True).to_csv('./data/country_codes.txt',sep='\\t', index=False),\n",
    "\n",
    "#---------------------------------------------------\n",
    "#2. 'created_at', 'launched_at', 'deadline', 'duration'\n",
    "# There is no discernible difference between created_at and launched_at since they are, at maximum, only few days apart in oorder to have an \n",
    "#effect on the results we look for. duration provides the difference in days between launched_at and deadline and we keep this parameter (for now).\n",
    "\n",
    "#-------------------------------------------------\n",
    "#3. 'currency', 'goal_in_local_currency', 'pledged_in_local_currency', 'usd_pledged', 'converted_pledged_amount_usd',\n",
    "# 'fx_rate', 'static_usd_rate', 'usd_exchange_rate'\n",
    "# There is the goal- but only in local currency- and the pledged amount- in both local currency and usd. \n",
    "# We add a new column, 'goal_in_usd', which gives the goal in usd as well. It is obtained by multiplying the 'goal_in_local_currency' with\n",
    "# the provided 'usd_exchange_rate' (Logic: The converted_pledged_amount_usd is provided by the author as a product of 'usd_exchange_rate' \n",
    "# and pledged_i'n_local_currency).\n",
    "df['goal_usd']= df['goal_in_local_currency']*df['usd_exchange_rate'] \n",
    "#We retain, in the end, 'goal_in_usd' and 'converted_pledged_amount_usd' and drop other currency, exchange rates and goal and pledged amounts\n",
    "#in local currency.\n",
    "\n",
    "#-------------------------------------------------\n",
    "#4. 'staff_pick' and 'state.1'\n",
    "# These columns are dropped, since state.1 is a reptition of the column 'is_success' and 'staff_pick' do not seem to add value to the \n",
    "#analysis at hand.\n",
    "\n",
    "#-------------------------------------------------\n",
    "#5. 'slug' and 'campaign_name'\n",
    "# 'slug'is a repetition of 'campaign_name', it is dropped.\n",
    "\n",
    "#-------------------------------------------------\n",
    "#Drop unwanted columns\n",
    "columns_to_drop= ['country_displayable_name', \n",
    "       'slug',\n",
    "       'created_at', \n",
    "       'launched_at', \n",
    "       'deadline', \n",
    "       'currency', \n",
    "       'goal_in_local_currency', \n",
    "       'pledged_in_local_currency', \n",
    "       'usd_pledged',\n",
    "       'staff_pick', \n",
    "       'state.1', \n",
    "       'fx_rate', \n",
    "       'static_usd_rate', \n",
    "       'usd_exchange_rate',]\n",
    "for column in columns_to_drop:\n",
    "       if column in df.columns:\n",
    "              df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "#-------------------------------------------------\n",
    "#Reorder columns\n",
    "df=df[['campaign_name', \n",
    "       'blurb',\n",
    "       'main_category', \n",
    "       'sub_category', \n",
    "       'is_environmental', \n",
    "       'is_social', \n",
    "       'country', \n",
    "       'duration_in_days', \n",
    "       'goal_usd',\n",
    "       'pledged_amount_usd', \n",
    "       'is_success', \n",
    "       ]]\n",
    "#Round floating number values to 2\n",
    "df=df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of the selected features:\n",
      " campaign_name\n",
      "blurb\n",
      "main_category\n",
      "sub_category\n",
      "is_environmental\n",
      "is_social\n",
      "country\n",
      "duration_in_days\n",
      "goal_usd\n",
      "pledged_amount_usd\n",
      "is_success\n"
     ]
    }
   ],
   "source": [
    "print('Overview of the selected features:\\n','\\n'.join(list(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also drop the rows which have 'NaN' values in more than 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df.isna().sum(axis=1) <= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we remove those rows which do not have the 'campaign_name','blurb', 'main_category', 'sub_category', in the expected string format.\n",
    "\n",
    "We also remove the rows where the columns 'duration_in_days','goal_usd', 'pledged_amount_usd' also do not have data in the expected number format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns= ['campaign_name','blurb', 'main_category', 'sub_category']\n",
    "is_str= df[columns].map(lambda x: isinstance(x, str))\n",
    "all_str= is_str.all(axis=1)\n",
    "df=df[all_str]\n",
    "\n",
    "columns= ['duration_in_days','goal_usd', 'pledged_amount_usd']\n",
    "is_nbr= df[columns].map(lambda x: isinstance(x, (int,float)))\n",
    "all_nbr= is_nbr.all(axis=1)\n",
    "df=df[all_nbr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also consider only those rows which are in english language. We first remove rows whose descriptions do not appear in latin script. Further more, we delete rows which contain only links. We also try to deduce the language of the description and retain only those rows whose description is provided in english.\n",
    "Note:\n",
    "- The rows containing only links for description are found using a url_regex. It is not perfect and cannot detect all rows with urls only. We resort to manualk deletion of such rows (In our case: 1 row only).\n",
    "- The detection of languages is implemented by the [langdetect](https://pypi.org/project/langdetect/) package. It also provide false negatives. But since these are negligeble compared to the total data corpus, we disregard the false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that the descriptions contain only latin script\n",
    "df['sane_descriptors']= df[['blurb','campaign_name']].apply(lambda row: 'yes' if bool(re.match('^(?=.*[a-zA-Z])', row['blurb'])) and bool(re.match('^(?=.*[a-zA-Z])', row['campaign_name'])) else 'no', axis= 1)\n",
    "df = df[df['sane_descriptors'] == 'yes']\n",
    "df.drop('sane_descriptors', axis=1, inplace=True)\n",
    "\n",
    "#Remove rows which contain only links for descriptions\n",
    "url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "#warning: this does not capture all the links only, but majority of them.\n",
    "df= df[~df['blurb'].str.contains('^'+url_regex+'$', na=False)]\n",
    "\n",
    "#Some rows are removed manually, so that it does not interfere with \n",
    "#the detect()function which captures the language.\n",
    "df=df.drop(10795,axis=0) #The description has only links chained after one another and not captured by url_regex\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "#Implement langdetect in batches\n",
    "batch_size= 5000\n",
    "num_batches= (len(df)//batch_size)+1\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_index= i*batch_size\n",
    "    end_index= (i+1)*batch_size if (i+1)*batch_size < len(df) else len(df)\n",
    "    \n",
    "    print(f'Batch: {i}, Rows: {start_index}-{end_index}')\n",
    "    df.loc[start_index:end_index, 'language']= df.loc[start_index:end_index]['blurb'].apply(\n",
    "        lambda row: detect(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check if there are any unprocessed rows. If yes, detect the language in these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed samples: 176388\n",
      "Empty/Unprocessed values: 0\n",
      "Samples with non-english descriptions: 7906\n"
     ]
    }
   ],
   "source": [
    "print(f'''Processed samples: {df['language'].value_counts().sum()}\n",
    "Empty/Unprocessed values: {df['language'].isna().sum()}\n",
    "Samples with non-english descriptions: {len(df[df['language']!='en'])}''')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the rows whose descriptions are not in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final dataset has 168482 samples.\n"
     ]
    }
   ],
   "source": [
    "#df[df['language']!='en'].sample(50)\n",
    "#print(f'There are {df[df['language']!='en'].shape[0]} samples with non-english descriptions.')\n",
    "df= df[df['language']=='en']\n",
    "df.drop('language', axis= 1, inplace=True)\n",
    "#Reset index\n",
    "df= df.reset_index(drop=True)\n",
    "print(f'The final dataset has {len(df)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save-dataset\"></a>\n",
    "### Save dataset\n",
    "--- -------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we save the data to a local file for the next set of analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the processed data\n",
    "#df.dropna(how='all', inplace=True)\n",
    "#df.to_csv('./data/dataframe_stripped_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook 01.Dataset_Prep.ipynb to webpdf\n",
      "[NbConvertApp] Building PDF\n",
      "[NbConvertApp] PDF successfully created\n",
      "[NbConvertApp] Writing 69316 bytes to 01.Dataset_Prep.pdf\n",
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-2' coro=<Connection.run() running at C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\site-packages\\playwright\\_impl\\_connection.py:274> wait_for=<Future pending cb=[Task.task_wakeup()]>>\n",
      "Exception ignored in: <function _ProactorBasePipeTransport.__del__ at 0x00000206FA41DA80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\proactor_events.py\", line 116, in __del__\n",
      "    _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "                               ^^^^^^^^\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\proactor_events.py\", line 80, in __repr__\n",
      "    info.append(f'fd={self._sock.fileno()}')\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\windows_utils.py\", line 102, in fileno\n",
      "    raise ValueError(\"I/O operation on closed pipe\")\n",
      "ValueError: I/O operation on closed pipe\n",
      "Exception ignored in: <function BaseSubprocessTransport.__del__ at 0x00000206FA41C220>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\base_subprocess.py\", line 125, in __del__\n",
      "    _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "                               ^^^^^^^^\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\base_subprocess.py\", line 70, in __repr__\n",
      "    info.append(f'stdin={stdin.pipe}')\n",
      "                        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\proactor_events.py\", line 80, in __repr__\n",
      "    info.append(f'fd={self._sock.fileno()}')\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\windows_utils.py\", line 102, in fileno\n",
      "    raise ValueError(\"I/O operation on closed pipe\")\n",
      "ValueError: I/O operation on closed pipe\n",
      "Exception ignored in: <function _ProactorBasePipeTransport.__del__ at 0x00000206FA41DA80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\proactor_events.py\", line 116, in __del__\n",
      "    _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "                               ^^^^^^^^\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\proactor_events.py\", line 80, in __repr__\n",
      "    info.append(f'fd={self._sock.fileno()}')\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ronin\\miniforge3\\envs\\dl4cv\\Lib\\asyncio\\windows_utils.py\", line 102, in fileno\n",
      "    raise ValueError(\"I/O operation on closed pipe\")\n",
      "ValueError: I/O operation on closed pipe\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to webpdf 01.Dataset_Prep.ipynb --no-input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
